{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lc_net.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QZtdOQm4oAu1",
        "ya3ZmsSUqtd3"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.9 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "87ae7d1e75b14a98f2d7b99b6b39b40721989d38e6517f9dbec64ca4d8e3011b"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "661194ee80564effb4e50a9317318c7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_92194849783e4c0b960c23ca86006a10",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_98fc35d9ecca410f8b515b11c7260b43",
              "IPY_MODEL_fdda91ea19b3465a9ccf453cc2a36c15",
              "IPY_MODEL_7a08fbda807c40c79b41433ccc759803"
            ]
          }
        },
        "92194849783e4c0b960c23ca86006a10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "98fc35d9ecca410f8b515b11c7260b43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_672be7dc1f1041ecbe4d8e6d2e6554e3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Running accuracy: 100.00%, Current val accuracy: 0.00%, :   1%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ea4a9292abf5464ca25e407a262b7854"
          }
        },
        "fdda91ea19b3465a9ccf453cc2a36c15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_db144e915cb14c418d82b2f9107cef01",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 2000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 14,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b69ab972c8a842f9aacebba80d5554b7"
          }
        },
        "7a08fbda807c40c79b41433ccc759803": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6bf1b8ee9ecc4330a35777de7a491df5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 14/2000 [00:35&lt;1:23:37,  2.53s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9042967f47494e5a92663c30648fcb74"
          }
        },
        "672be7dc1f1041ecbe4d8e6d2e6554e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ea4a9292abf5464ca25e407a262b7854": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "db144e915cb14c418d82b2f9107cef01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b69ab972c8a842f9aacebba80d5554b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6bf1b8ee9ecc4330a35777de7a491df5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9042967f47494e5a92663c30648fcb74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Singular-Brain/bindsnet/blob/master/BioLCNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fTSvrK3T_GA"
      },
      "source": [
        "#Notebook setups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXtgP_iEPE0G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5044bb72-8581-4946-e0de-1ebd4216c8ec"
      },
      "source": [
        "!pip install -q git+https://github.com/Singular-Brain/bindsnet"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |██▊                             | 10 kB 38.8 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 20 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 30 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 40 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 51 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 61 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 71 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 81 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 92 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 102 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 112 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 120 kB 7.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 73 kB 2.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 280 kB 70.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 28.5 MB 41 kB/s \n",
            "\u001b[?25h  Building wheel for BindsNET (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW7m3ugEHZP_",
        "outputId": "a9d2034a-dfb9-4895-b8d4-e1551d942c37"
      },
      "source": [
        "!wget https://data.deepai.org/mnist.zip\n",
        "!mkdir -p ../data/MNIST/TorchvisionDatasetWrapper/raw\n",
        "!unzip mnist.zip -d ../data/MNIST/TorchvisionDatasetWrapper/raw/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-05 19:32:52--  https://data.deepai.org/mnist.zip\n",
            "Resolving data.deepai.org (data.deepai.org)... 138.201.36.183\n",
            "Connecting to data.deepai.org (data.deepai.org)|138.201.36.183|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11597176 (11M) [application/x-zip-compressed]\n",
            "Saving to: ‘mnist.zip’\n",
            "\n",
            "mnist.zip           100%[===================>]  11.06M  8.15MB/s    in 1.4s    \n",
            "\n",
            "2021-09-05 19:32:54 (8.15 MB/s) - ‘mnist.zip’ saved [11597176/11597176]\n",
            "\n",
            "Archive:  mnist.zip\n",
            "  inflating: ../data/MNIST/TorchvisionDatasetWrapper/raw/train-labels-idx1-ubyte.gz  \n",
            "  inflating: ../data/MNIST/TorchvisionDatasetWrapper/raw/train-images-idx3-ubyte.gz  \n",
            "  inflating: ../data/MNIST/TorchvisionDatasetWrapper/raw/t10k-images-idx3-ubyte.gz  \n",
            "  inflating: ../data/MNIST/TorchvisionDatasetWrapper/raw/t10k-labels-idx1-ubyte.gz  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXcXvvsXcOlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "952db081-714a-43bf-ff7d-ba4f46484263"
      },
      "source": [
        "!git clone https://github.com/Singular-Brain/bindsnet/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bindsnet'...\n",
            "remote: Enumerating objects: 11081, done.\u001b[K\n",
            "remote: Counting objects: 100% (2041/2041), done.\u001b[K\n",
            "remote: Compressing objects: 100% (893/893), done.\u001b[K\n",
            "remote: Total 11081 (delta 1439), reused 1639 (delta 1147), pack-reused 9040\u001b[K\n",
            "Receiving objects: 100% (11081/11081), 86.69 MiB | 22.06 MiB/s, done.\n",
            "Resolving deltas: 100% (7165/7165), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFGNAecpT-Lj"
      },
      "source": [
        "from bindsnet.network.nodes import Nodes\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import copy\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from torchvision import transforms\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import torch.nn.functional as fn\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Union, Tuple, Optional, Sequence\n",
        "from torch.nn.modules.utils import _pair\n",
        "\n",
        "from bindsnet.datasets import MNIST\n",
        "from bindsnet.encoding import PoissonEncoder\n",
        "from bindsnet.network import Network\n",
        "from bindsnet.network.nodes import Input, LIFNodes, AdaptiveLIFNodes, IFNodes\n",
        "from bindsnet.network.topology import LocalConnection, Connection, LocalConnectionOrig, MaxPool2dLocalConnection\n",
        "from bindsnet.network.monitors import Monitor, AbstractMonitor, TensorBoardMonitor\n",
        "from bindsnet.learning import PostPre, MSTDP, MSTDPET, WeightDependentPostPre, Hebbian\n",
        "from bindsnet.learning.reward import DynamicDopamineInjection, DopaminergicRPE\n",
        "from bindsnet.analysis.plotting import plot_locally_connected_weights,plot_locally_connected_weights_meh,plot_spikes,plot_locally_connected_weights_meh2,plot_convergence_and_histogram,plot_locally_connected_weights_meh3\n",
        "from bindsnet.analysis.visualization import plot_weights_movie, plot_spike_trains_for_example,summary, plot_voltage\n",
        "from bindsnet.utils import reshape_locally_connected_weights, reshape_locally_connected_weights_meh, reshape_conv2d_weights"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULGGHW43UksI"
      },
      "source": [
        "## Sets up Gpu use and manual seed\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiUmFrpcUfmR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0894ba85-f802-456c-ce47-f1d984077464"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device =  torch.device(\"cuda\")\n",
        "    gpu = True\n",
        "else:\n",
        "    device =  torch.device(\"cpu\")\n",
        "    gpu = False\n",
        "\n",
        "def manual_seed(seed):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "SEED = 2045 # The Singularity is Near!\n",
        "manual_seed(SEED)\n",
        "\n",
        "torch.set_num_threads(os.cpu_count() - 1)\n",
        "print(\"Running on Device = \", device)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on Device =  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBKedMpIleMr"
      },
      "source": [
        "# Custom Monitors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tfqpsr2a1WV"
      },
      "source": [
        "## Reward Monitor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M44GJ65GleMs"
      },
      "source": [
        "class RewardMonitor(AbstractMonitor):\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Records state variables of interest.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        time: None,\n",
        "        batch_size: int = 1,\n",
        "        device: str = \"cpu\",\n",
        "    ):\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Constructs a ``Monitor`` object.\n",
        "\n",
        "        :param obj: An object to record state variables from during network simulation.\n",
        "        :param state_vars: Iterable of strings indicating names of state variables to record.\n",
        "        :param time: If not ``None``, pre-allocate memory for state variable recording.\n",
        "        :param device: Allow the monitor to be on different device separate from Network device\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.time = time\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "\n",
        "        # if time is not specified the monitor variable accumulate the logs\n",
        "        if self.time is None:\n",
        "            self.device = \"cpu\"\n",
        "\n",
        "        self.recording = []\n",
        "        self.reset_state_variables()\n",
        "\n",
        "    def get(self,) -> torch.Tensor:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Return recording to user.\n",
        "\n",
        "        :return: Tensor of shape ``[time, n_1, ..., n_k]``, where ``[n_1, ..., n_k]`` is the shape of the recorded state\n",
        "        variable.\n",
        "        Note, if time == `None`, get return the logs and empty the monitor variable\n",
        "\n",
        "        \"\"\"\n",
        "        # return_logs = torch.as_tensor(self.recording)\n",
        "        # if self.time is None:\n",
        "        #     self.recording = []\n",
        "        return self.recording\n",
        "\n",
        "    def record(self, **kwargs) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Appends the current value of the recorded state variables to the recording.\n",
        "        \"\"\"\n",
        "        if \"reward\" in kwargs:\n",
        "            self.recording.append(kwargs[\"reward\"])\n",
        "        # remove the oldest element (first in the list)\n",
        "        # if self.time is not None:\n",
        "        #     self.recording.pop(0)\n",
        "\n",
        "    def reset_state_variables(self) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Resets recordings to empty ``List``s.\n",
        "        \"\"\"\n",
        "        self.recording = []\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8clxN_npa1WY"
      },
      "source": [
        "## Plot Eligibility trace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SshGlRwpa1WZ"
      },
      "source": [
        "class PlotET(AbstractMonitor):\n",
        "    # language=rst\n",
        "    \"\"\"\n",
        "    Records state variables of interest.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        i,\n",
        "        j,\n",
        "        source,\n",
        "        target,\n",
        "        connection,\n",
        "    ):\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Constructs a ``Monitor`` object.\n",
        "\n",
        "        :param obj: An object to record state variables from during network simulation.\n",
        "        :param state_vars: Iterable of strings indicating names of state variables to record.\n",
        "        :param time: If not ``None``, pre-allocate memory for state variable recording.\n",
        "        :param device: Allow the monitor to be on different device separate from Network device\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.i = i\n",
        "        self.j = j\n",
        "        self.source = source\n",
        "        self.target = target\n",
        "        self.connection = connection\n",
        "\n",
        "        self.reset_state_variables()\n",
        "\n",
        "    def get(self,) -> torch.Tensor:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Return recording to user.\n",
        "\n",
        "        :return: Tensor of shape ``[time, n_1, ..., n_k]``, where ``[n_1, ..., n_k]`` is the shape of the recorded state\n",
        "        variable.\n",
        "        Note, if time == `None`, get return the logs and empty the monitor variable\n",
        "\n",
        "        \"\"\"\n",
        "        # return_logs = torch.as_tensor(self.recording)\n",
        "        # if self.time is None:\n",
        "        #     self.recording = []\n",
        "        return self.recording\n",
        "\n",
        "    def record(self, **kwargs) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Appends the current value of the recorded state variables to the recording.\n",
        "        \"\"\"\n",
        "        if hasattr(self.connection.update_rule, 'p_plus'):\n",
        "            self.recording['spikes_i'].append(self.source.s.ravel()[self.i].item())\n",
        "            self.recording['spikes_j'].append(self.target.s.ravel()[self.j].item())\n",
        "            self.recording['p_plus'].append(self.connection.update_rule.p_plus[self.i].item())\n",
        "            self.recording['p_minus'].append(self.connection.update_rule.p_minus[self.j].item())\n",
        "            self.recording['eligibility'].append(self.connection.update_rule.eligibility[self.i,self.j].item())\n",
        "            self.recording['eligibility_trace'].append(self.connection.update_rule.eligibility_trace[self.i,self.j].item())\n",
        "            self.recording['w'].append(self.connection.w[self.i,self.j].item())\n",
        "\n",
        "    def plot(self):\n",
        "\n",
        "        fig, axs  = plt.subplots(7)\n",
        "        fig.set_size_inches(10, 20)\n",
        "        for i, (name, p) in enumerate(self.recording.items()):\n",
        "            axs[i].plot(p[-250:])\n",
        "            axs[i].set_title(name)\n",
        "    \n",
        "        fig.show()\n",
        "\n",
        "    def reset_state_variables(self) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Resets recordings to empty ``List``s.\n",
        "        \"\"\"\n",
        "        self.recording = {\n",
        "        'spikes_i': [],\n",
        "        'spikes_j': [],\n",
        "        'p_plus':[],\n",
        "        'p_minus':[],\n",
        "        'eligibility':[],\n",
        "        'eligibility_trace':[],\n",
        "        'w': [],\n",
        "        }\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_YGE1XjvIkZ"
      },
      "source": [
        "## Kernel "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4hp2V46vOUv"
      },
      "source": [
        "class AbstractKernel(ABC):\n",
        "    def __init__(self, kernel_size):\n",
        "        \"\"\"\n",
        "        Base class for generating image filter kernels such as Gabor, DoG, etc. Each subclass should override :attr:`__call__` function.\n",
        "        Instantiates a ``Filter Kernel`` object.\n",
        "        :param window_size : The size of the kernel (int)\n",
        "        \"\"\"\n",
        "        self.window_size = kernel_size\n",
        "\n",
        "    def __call__(self):\n",
        "        pass\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL2L6_ABwBH4"
      },
      "source": [
        "class DoGKernel(AbstractKernel):\n",
        "    def __init__(self, kernel_size: Union[int, Tuple[int, int]], sigma1 : float, sigma2 : float):\n",
        "        \"\"\"\n",
        "        Generates DoG filter kernels.\n",
        "        :param kernel_size: Horizontal and vertical size of DOG kernels.(If pass int, we consider it as a square filter) \n",
        "        :param sigma1 : The sigma parameter for the first Gaussian function.\n",
        "        :param sigma2 : The sigma parameter for the second Gaussian function.\n",
        "        \"\"\"\n",
        "        super(DoGKernel, self).__init__(kernel_size)\n",
        "        self.sigma1 = sigma1\n",
        "        self.sigma2 = sigma2\n",
        "        \n",
        "    def __call__(self):\n",
        "        k = self.window_size//2\n",
        "        x, y = np.mgrid[-k:k+1:1, -k:k+1:1]\n",
        "        a = 1.0 / (2 * math.pi)\n",
        "        prod = x*x + y*y\n",
        "        f1 = (1/(self.sigma1*self.sigma1)) * np.exp(-0.5 * (1/(self.sigma1*self.sigma1)) * (prod))\n",
        "        f2 = (1/(self.sigma2*self.sigma2)) * np.exp(-0.5 * (1/(self.sigma2*self.sigma2)) * (prod))\n",
        "        dog = a * (f1-f2)\n",
        "        dog_mean = np.mean(dog)\n",
        "        dog = dog - dog_mean\n",
        "        dog_max = np.max(dog)\n",
        "        dog = dog / dog_max\n",
        "        dog_tensor = torch.from_numpy(dog)\n",
        "        # returns a 2d tensor corresponding to the requested DoG filter\n",
        "        return dog_tensor.float()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBUT0IUZDXxW"
      },
      "source": [
        "class Filter():\n",
        "    \"\"\"\n",
        "    Applies a filter transform. Each filter contains a sequence of :attr:`FilterKernel` objects.\n",
        "    The result of each filter kernel will be passed through a given threshold (if not :attr:`None`).\n",
        "    Args:\n",
        "        filter_kernels (sequence of FilterKernels): The sequence of filter kernels.\n",
        "        padding (int, optional): The size of the padding for the convolution of filter kernels. Default: 0\n",
        "        thresholds (sequence of floats, optional): The threshold for each filter kernel. Default: None\n",
        "        use_abs (boolean, optional): To compute the absolute value of the outputs or not. Default: False\n",
        "    .. note::\n",
        "        The size of the compund filter kernel tensor (stack of individual filter kernels) will be equal to the \n",
        "        greatest window size among kernels. All other smaller kernels will be zero-padded with an appropriate \n",
        "        amount.\n",
        "    \"\"\"\n",
        "    # filter_kernels must be a list of filter kernels\n",
        "    # thresholds must be a list of thresholds for each kernel\n",
        "    def __init__(self, filter_kernels, padding=0, thresholds=None, use_abs=False):\n",
        "        tensor_list = []\n",
        "        self.max_window_size = 0\n",
        "        for kernel in filter_kernels:\n",
        "            if isinstance(kernel, torch.Tensor):\n",
        "                tensor_list.append(kernel)\n",
        "                self.max_window_size = max(self.max_window_size, kernel.size(-1))\n",
        "            else:\n",
        "                tensor_list.append(kernel().unsqueeze(0))\n",
        "                self.max_window_size = max(self.max_window_size, kernel.window_size)\n",
        "        for i in range(len(tensor_list)):\n",
        "            p = (self.max_window_size - filter_kernels[i].window_size)//2\n",
        "            tensor_list[i] = fn.pad(tensor_list[i], (p,p,p,p))\n",
        "\n",
        "        self.kernels = torch.stack(tensor_list)\n",
        "        self.number_of_kernels = len(filter_kernels)\n",
        "        self.padding = padding\n",
        "        if isinstance(thresholds, list):\n",
        "            self.thresholds = thresholds.clone().detach()\n",
        "            self.thresholds.unsqueeze_(0).unsqueeze_(2).unsqueeze_(3)\n",
        "        else:\n",
        "            self.thresholds = thresholds\n",
        "        self.use_abs = use_abs\n",
        "\n",
        "    # returns a 4d tensor containing the flitered versions of the input image\n",
        "    # input is a 4d tensor. dim: (minibatch=1, filter_kernels, height, width)\n",
        "    def __call__(self, input):\n",
        "\n",
        "        # if input.dim() == 3:\n",
        "        #     input2 = torch.unsqueeze(input, 0)\n",
        "        input.unsqueeze_(0)\n",
        "        output = fn.conv2d(input, self.kernels, padding = self.padding).float()\n",
        "        if not(self.thresholds is None):\n",
        "            output = torch.where(output < self.thresholds, torch.tensor(0.0, device=output.device), output)\n",
        "        if self.use_abs:\n",
        "            torch.abs_(output)\n",
        "        return output.squeeze(0)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywXyWP0I83Au"
      },
      "source": [
        "# Design network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bZpJmlrJDa9"
      },
      "source": [
        "compute_size = lambda inp_size, k, s: int((inp_size-k)/s) + 1\n",
        "\n",
        "class LCNet(Network):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_classes: int,\n",
        "        neuron_per_class: int,\n",
        "        in_channels : int,\n",
        "        n_channels1: int,\n",
        "        n_channels2: int,\n",
        "        filter_size1: int,\n",
        "        filter_size2: int,\n",
        "        stride1: int,\n",
        "        stride2: int,\n",
        "        maxPool1: bool,\n",
        "        maxPool2: bool,\n",
        "        online: bool,\n",
        "        deep: bool,\n",
        "        time: int,\n",
        "        reward_fn,\n",
        "        n_neurons: int,\n",
        "        pre_observation: bool,\n",
        "        has_decision_period: bool,\n",
        "        local_rewarding: bool,\n",
        "        nu_LC: Union[float, Tuple[float, float]],\n",
        "        nu_LC2: Union[float, Tuple[float, float]],\n",
        "        nu_Output: float,\n",
        "        dt: float,\n",
        "        crop_size:int ,\n",
        "        nu_inh_LC: float,\n",
        "        nu_inh: float,\n",
        "        inh_type,\n",
        "        inh_LC: bool,\n",
        "        inh_LC2: bool,\n",
        "        inh_factor_LC: float,\n",
        "        inh_factor_LC2: float,\n",
        "        inh_factor:float,\n",
        "        single_output_layer:bool,\n",
        "        NodesType_LC,\n",
        "        NodesType_Output, \n",
        "        update_rule_LC,\n",
        "        update_rule_LC2,\n",
        "        update_rule_Output,\n",
        "        update_rule_inh,\n",
        "        update_rule_inh_LC,\n",
        "        wmin: float,\n",
        "        wmax: float ,\n",
        "        soft_bound,\n",
        "        theta_plus: float,\n",
        "        tc_theta_decay: float,\n",
        "        tc_trace:int,\n",
        "        normal_init:bool,\n",
        "        mu: float,\n",
        "        std:float,\n",
        "        norm_factor_inh_LC: bool,\n",
        "        norm_factor_LC,\n",
        "        norm_factor_LC2,\n",
        "        norm_factor_out,\n",
        "        norm_factor_inh,\n",
        "        trace_additive,\n",
        "        load_path,\n",
        "        save_path,\n",
        "        LC_weights_path,\n",
        "        LC2_weights_path,\n",
        "        confusion_matrix,\n",
        "        lc_weights_vis,\n",
        "        out_weights_vis,\n",
        "        lc_convergence_vis,\n",
        "        out_convergence_vis,\n",
        "        thresh_LC,\n",
        "        thresh_FC,\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        # language=rst\n",
        "        \"\"\"\n",
        "        Constructor for class ``BioLCNet``.\n",
        "\n",
        "        :param n_inpt: Number of input neurons. Matches the 1D size of the input data.\n",
        "        :param n_neurons: Number of excitatory, inhibitory neurons.\n",
        "        :param exc: Strength of synapse weights from excitatory to inhibitory layer.\n",
        "        :param inh: Strength of synapse weights from inhibitory to excitatory layer.\n",
        "        :param dt: Simulation time step.\n",
        "        :param nu: Single or pair of learning rates for pre- and post-synaptic events,\n",
        "            respectively.\n",
        "        :param reduction: Method for reducing parameter updates along the minibatch\n",
        "            dimension.\n",
        "        :param wmin: Minimum allowed weight on input to excitatory synapses.\n",
        "        :param wmax: Maximum allowed weight on input to excitatory synapses.\n",
        "        :param norm: Input to excitatory layer connection weights normalization\n",
        "            constant.\n",
        "        :param theta_plus: On-spike increment of ``(adaptive)LIFNodes`` membrane\n",
        "            threshold potential.\n",
        "        :param tc_theta_decay: Time constant of ``(adaptive)LIFNodes`` threshold\n",
        "            potential decay.\n",
        "        :param inpt_shape: The dimensionality of the input layer.\n",
        "        \"\"\"\n",
        "        manual_seed(SEED)\n",
        "        super().__init__(dt=dt, reward_fn = None, online=online)\n",
        "        kwargs['single_output_layer'] = single_output_layer\n",
        "        kwargs['dt'] = dt\n",
        "        kwargs['n_labels'] = n_classes\n",
        "        kwargs['neuron_per_class'] = neuron_per_class\n",
        "        \n",
        "        self.dt = dt\n",
        "        self.reward_fn = reward_fn(**kwargs)\n",
        "        self.reward_fn.network = self\n",
        "        self.reward_fn.dt = self.dt\n",
        "        self.n_classes = n_classes\n",
        "        self.neuron_per_class = neuron_per_class\n",
        "        self.save_path = save_path\n",
        "        self.load_path = load_path\n",
        "        self.deep = deep\n",
        "        self.maxPool1 = maxPool1\n",
        "        self.maxPool2 = maxPool2\n",
        "        self.time = time\n",
        "        self.crop_size = crop_size\n",
        "        self.filter_size1 = filter_size1\n",
        "        self.filter_size2 = filter_size2\n",
        "        self.clamp_intensity = kwargs.get('clamp_intensity',None)\n",
        "        self.single_output_layer = single_output_layer\n",
        "        self.pre_observation = pre_observation\n",
        "        self.has_decision_period = has_decision_period\n",
        "        self.local_rewarding = local_rewarding\n",
        "        self.soft_bound = soft_bound\n",
        "        self.confusion_matrix = confusion_matrix\n",
        "        self.lc_weights_vis = lc_weights_vis\n",
        "        self.out_weights_vis = out_weights_vis\n",
        "        self.lc_convergence_vis = lc_convergence_vis\n",
        "        self.out_convergence_vis = out_convergence_vis\n",
        "        self.in_channels = in_channels\n",
        "        self.n_channels1 = n_channels1\n",
        "        self.n_channels2 = n_channels2\n",
        "        self.convergences = {}\n",
        "        self.norm_factor_LC = norm_factor_LC\n",
        "        self.norm_factor_LC2 = norm_factor_LC2\n",
        "        self.norm_factor_out = norm_factor_out\n",
        "        self.wmin = wmin \n",
        "        self.wmax = wmax\n",
        "\n",
        "        if kwargs['variant'] == 'scalar':\n",
        "            assert self.has_decision_period == True, ''\n",
        "\n",
        "        if self.online == False:\n",
        "            assert self.has_decision_period == True, ''\n",
        "        \n",
        "        if self.has_decision_period == True:\n",
        "            assert self.online == False, \"Decision period is not compatible with online learning.\"\n",
        "            self.observation_period = kwargs['observation_period']\n",
        "            assert self.observation_period >= 0, \"\"\n",
        "            self.decision_period = kwargs['decision_period']\n",
        "            assert self.decision_period > 0, \"\"\n",
        "            self.learning_period = self.time - self.observation_period - self.decision_period\n",
        "\n",
        "        elif self.pre_observation == True:\n",
        "            self.observation_period = kwargs['observation_period']\n",
        "            assert self.observation_period >= 0, \"\"\n",
        "            self.learning_period = self.time - self.observation_period\n",
        "            self.decision_period = self.time - self.observation_period\n",
        "\n",
        "        else:\n",
        "            self.observation_period = 0\n",
        "            self.decision_period = self.time\n",
        "            self.learning_period = self.time\n",
        "\n",
        "        ### nodes\n",
        "        inp = Input(shape= [in_channels,crop_size,crop_size], traces=True, tc_trace=tc_trace,traces_additive = trace_additive)\n",
        "        self.add_layer(inp, name=\"input\")\n",
        "\n",
        "        ## First hidden layer\n",
        "        conv_size1 = compute_size(crop_size, filter_size1, stride1)\n",
        "        main1 = NodesType_LC(shape= [n_channels1, conv_size1, conv_size1], thresh = thresh_LC, traces=True, tc_trace=tc_trace,\n",
        "                             traces_additive = trace_additive,tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "        \n",
        "        self.add_layer(main1, name=\"main1\")\n",
        "\n",
        "        ### connections \n",
        "        LC1 = LocalConnection(inp, main1, filter_size1, stride1, in_channels, n_channels1, nu = _pair(nu_LC), update_rule = update_rule_LC,wmin = wmin, wmax= wmax, soft_bound = soft_bound, norm = norm_factor_LC)\n",
        "\n",
        "        if LC_weights_path:\n",
        "            a = torch.load(LC_weights_path)\n",
        "            LC1.w.data = a['state_dict']['input_to_main1.w']\n",
        "            print(\"Weights loaded ...\")\n",
        "        \n",
        "        elif normal_init:\n",
        "            w_lc_init = torch.normal(mu,std, size = (in_channels, n_channels1 * compute_size(crop_size, filter_size1, stride1)**2, filter_size1**2))\n",
        "            LC1.w.data = w_lc_init\n",
        "       \n",
        "        self.add_connection(LC1, \"input\", \"main1\")\n",
        "        self.convergences['lc1'] = []\n",
        "\n",
        "        if inh_LC:\n",
        "            main_width = compute_size(crop_size, filter_size1, stride1)\n",
        "            w_inh_LC = torch.zeros(n_channels1,main_width,main_width,n_channels1,main_width,main_width)\n",
        "            for c in range(n_channels1):\n",
        "                for w1 in range(main_width):\n",
        "                    for w2 in range(main_width):\n",
        "                        w_inh_LC[c,w1,w2,:,w1,w2] = - inh_factor_LC\n",
        "                        w_inh_LC[c,w1,w2,c,w1,w2] = 0\n",
        "        \n",
        "            w_inh_LC = w_inh_LC.reshape(main1.n,main1.n)\n",
        "                                                             \n",
        "            LC_recurrent_inhibition = Connection(\n",
        "                source=main1,\n",
        "                target=main1,\n",
        "                w=w_inh_LC,\n",
        "            )\n",
        "            self.add_connection(LC_recurrent_inhibition, \"main1\", \"main1\")\n",
        "        \n",
        "        \n",
        "        self.final_connection_source_name = 'main1'\n",
        "        self.final_connection_source = main1\n",
        "\n",
        "        self.hidden2 = main1\n",
        "        self.hidden2_name = 'main1'\n",
        "        if maxPool1:\n",
        "            maxPool_kernel = 2\n",
        "            maxPool_stride = 2\n",
        "            \n",
        "            conv_size1 =compute_size(conv_size1, maxPool_kernel, maxPool_stride)\n",
        "            self.final_connection_source_name = 'maxpool1'\n",
        "            \n",
        "            maxpool1 = LIFNodes(shape= [self.n_channels1, conv_size1, conv_size1], refrac = 0)\n",
        "            self.add_layer(maxpool1, name=\"maxpool1\")\n",
        "            self.final_connection_source = maxpool1\n",
        "            \n",
        "            maxPoolConnection = MaxPool2dLocalConnection(main1, maxpool1, maxPool_kernel, maxPool_stride)\n",
        "            self.add_connection(maxPoolConnection, \"main1\", 'maxpool1')\n",
        "            \n",
        "            self.hidden2 = maxpool1\n",
        "            self.hidden2_name = 'maxpool1'\n",
        "\n",
        "        if deep:\n",
        "            # # Second hidden layer\n",
        "            conv_size2 = compute_size(conv_size1, filter_size2, stride2)\n",
        "\n",
        "            main2 = NodesType_LC(shape= [n_channels2, conv_size2, conv_size2],traces=True, tc_trace=tc_trace,traces_additive = trace_additive,\n",
        "                                            tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "            \n",
        "            self.add_layer(main2, name=\"main2\")\n",
        "\n",
        "            ### connections \n",
        "            lc2_input_shape = (conv_size1,conv_size1)\n",
        "            LC2 = LocalConnection(self.hidden2, main2, filter_size2, stride2, n_channels1, n_channels2, input_shape= lc2_input_shape,\n",
        "            nu = _pair(nu_LC2), update_rule = update_rule_LC2, wmin = wmin, wmax= wmax, soft_bound = soft_bound, norm = norm_factor_LC2)\n",
        "\n",
        "            self.add_connection(LC2,  self.hidden2_name, \"main2\")\n",
        "            self.convergences['lc2'] = []\n",
        "            if LC2_weights_path:\n",
        "                a = torch.load(LC2_weights_path)\n",
        "                LC2.w.data = a['state_dict']['main1_to_main2.w']\n",
        "                print(\"Weights loaded ...\")\n",
        "            \n",
        "            elif normal_init:\n",
        "                w_lc_init = torch.normal(mu,std, size = (n_channels1, n_channels2 * compute_size(conv_size1, filter_size2, stride2)**2, filter_size2**2))\n",
        "                LC2.w.data = w_lc_init\n",
        "\n",
        "            self.final_connection_source_name = 'main2'\n",
        "            self.final_connection_source = main2\n",
        "\n",
        "            if inh_LC2:\n",
        "                main_width = conv_size2\n",
        "                w_inh_LC2 = torch.zeros(n_channels2,main_width,main_width,n_channels2,main_width,main_width)\n",
        "                for c in range(n_channels2):\n",
        "                    for w1 in range(main_width):\n",
        "                        for w2 in range(main_width):\n",
        "                            w_inh_LC2[c,w1,w2,:,w1,w2] = - inh_factor_LC2\n",
        "                            w_inh_LC2[c,w1,w2,c,w1,w2] = 0\n",
        "            \n",
        "                w_inh_LC2 = w_inh_LC2.reshape(main2.n,main2.n)\n",
        "                                                                \n",
        "                LC_recurrent_inhibition2 = Connection(\n",
        "                    source=main2,\n",
        "                    target=main2,\n",
        "                    w=w_inh_LC2,\n",
        "                )\n",
        "                self.add_connection(LC_recurrent_inhibition2, \"main2\", \"main2\")\n",
        "\n",
        "\n",
        "            if maxPool2:\n",
        "                maxPool_kernel = 2\n",
        "                maxPool_stride = 2\n",
        "                conv_size2 =compute_size(conv_size2, maxPool_kernel, maxPool_stride)\n",
        "                self.final_connection_source_name = 'maxpool2'\n",
        "                maxpool2 = LIFNodes(shape= [self.n_channels2, conv_size2, conv_size2], refrac = 0)\n",
        "                self.final_connection_source = maxpool2\n",
        "                maxPoolConnection2 = MaxPool2dLocalConnection(main2, maxpool2, maxPool_kernel, maxPool_stride)\n",
        "\n",
        "                self.add_layer(maxpool2, name=\"maxpool2\")\n",
        "                self.add_connection(maxPoolConnection2, \"main2\", 'maxpool2')\n",
        "\n",
        "\n",
        "        ### main2 to output\n",
        "        out = NodesType_Output(n= n_neurons, traces=True,traces_additive = trace_additive, thresh=thresh_FC, tc_trace=tc_trace, tc_theta_decay = tc_theta_decay, theta_plus = theta_plus)\n",
        "\n",
        "        self.add_layer(out, \"output\")\n",
        "\n",
        "        last_main_out = Connection(self.final_connection_source, out, nu = nu_Output, update_rule = update_rule_Output, wmin = wmin, wmax= wmax, norm = norm_factor_out)\n",
        "\n",
        "        if normal_init:\n",
        "            w_last_main_init = torch.normal(mu,std,size = (self.final_connection_source.n,out.n)) \n",
        "            last_main_out.w.data = w_last_main_init\n",
        "        print(torch.mean(last_main_out.w),torch.std(last_main_out.w))    \n",
        "        self.add_connection(last_main_out, self.final_connection_source_name, \"output\")\n",
        "        self.convergences['last_main_out'] = []\n",
        "        ### Inhibitory:\n",
        "        if inh_type == 'between_layers':\n",
        "            w = -inh_factor * torch.ones(out.n, out.n)\n",
        "            for c in range(n_classes):\n",
        "                ind = slice(c*neuron_per_class,(c+1)*neuron_per_class)\n",
        "                w[ind, ind] = 0\n",
        "\n",
        "            out_recurrent_inhibition = Connection(\n",
        "                source=out,\n",
        "                target=out,\n",
        "                w=w,\n",
        "                update_rule = update_rule_inh,\n",
        "                wmin=-inh_factor,\n",
        "                wmax=0,\n",
        "                nu = nu_inh,\n",
        "                norm = norm_factor_inh,\n",
        "            )\n",
        "            self.add_connection(out_recurrent_inhibition, \"output\", \"output\")\n",
        "        elif inh_type == 'one_2_all':\n",
        "            w = -inh_factor * (torch.ones(out.n, out.n) - torch.eye(out.n, out.n))\n",
        "            out_recurrent_inhibition = Connection(\n",
        "                source=out,\n",
        "                target=out,\n",
        "                w=w,\n",
        "                update_rule = update_rule_inh,\n",
        "                wmin=-inh_factor,\n",
        "                wmax=0,\n",
        "                nu = nu_inh,\n",
        "                norm = norm_factor_inh,\n",
        "            )\n",
        "            self.add_connection(out_recurrent_inhibition, \"output\", \"output\")\n",
        "        # Diehl and Cook\n",
        "        elif inh_type == 'DC':\n",
        "            raise NotImplementedError('Diehl and cook not implemented yet fo r 10 classes')\n",
        "\n",
        "        # Directs network to GPU\n",
        "        if gpu:\n",
        "            self.to(\"cuda\")\n",
        "\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        dataloader,\n",
        "        val_loader,\n",
        "        reward_hparams,\n",
        "        label = None,\n",
        "        hparams = None,\n",
        "        online_validate = True,\n",
        "        n_train = 2000,\n",
        "        n_test = 250,\n",
        "        n_val = 250,\n",
        "        val_interval = 250,\n",
        "        running_window_length = 250,\n",
        "        verbose = True,\n",
        "    ):\n",
        "        manual_seed(SEED)\n",
        "        self.verbose = verbose\n",
        "        self.label = label\n",
        "        # add Monitors\n",
        "        #main_monitor = Monitor(self.layers[\"main\"], [\"v\"], time=None, device=device)\n",
        "        reward_monitor = RewardMonitor(time =self.time)\n",
        "        #Plot_et = PlotET(i = 0, j = 0, source = self.layers[\"main\"], target = self.layers[\"output\"], connection = self.connections[(\"main\",\"output\")])\n",
        "        #tensorboard = TensorBoardMonitor(self, time = self.time)\n",
        "        #self.add_monitor(main_monitor, name=\"main\")\n",
        "        self.add_monitor(reward_monitor, name=\"reward\")\n",
        "        #self.add_monitor(Plot_et, name=\"Plot_et\")\n",
        "        #self.add_monitor(tensorboard, name=\"tensorboard\")\n",
        "\n",
        "            \n",
        "        acc_hist = collections.deque([], running_window_length)\n",
        "\n",
        "        #if self.single_output_layer:\n",
        "        self.spikes = {}\n",
        "        for layer in set(self.layers):\n",
        "            self.spikes[layer] = Monitor(self.layers[layer], state_vars=[\"s\"], time=None)\n",
        "            self.add_monitor(self.spikes[layer], name=\"%s_spikes\" % layer)\n",
        "            self.dopaminergic_layers = self.layers[\"output\"]\n",
        "        # else:\n",
        "        #     output_layers = set([layer for layer in self.layers if layer.startswith('output')])\n",
        "        #     self.output_spikes = {}\n",
        "        #     for layer in output_layers:\n",
        "        #         self.output_spikes[layer] = Monitor(self.layers[layer], state_vars=[\"s\"], time=self.time)\n",
        "        #         self.add_monitor(self.output_spikes[layer], name=\"%s_spikes\" % layer)\n",
        "        #         self.dopaminergic_layers = {name: layer for name, layer in self.layers.items() if name.startswith('output')}\n",
        "\n",
        "        val_acc = 0.0\n",
        "        acc = 0.0\n",
        "\n",
        "        reward_history = []\n",
        "        if self.load_path:\n",
        "            # try:\n",
        "            self.model_params = torch.load(self.load_path)\n",
        "            self.load_state_dict(torch.load(self.load_path)['state_dict'])\n",
        "            iteration =  self.model_params['iteration']\n",
        "            hparams = self.model_params['hparams']\n",
        "            train_accs = self.model_params['train_accs']\n",
        "            val_accs = self.model_params['val_accs']\n",
        "            acc_rewards = self.model_params['acc_rewards']\n",
        "            print(f'Previous model loaded! Resuming training from iteration {iteration}..., last running training accuracy: {train_accs[-1]}, last validation accuracy: {val_accs[-1]}\\n') if self.verbose else None\n",
        "        else:\n",
        "            print(f'Previous model not found! Training from the beginning...\\n') if self.verbose else None\n",
        "            val_accs = []\n",
        "            train_accs = []\n",
        "            acc_rewards = []\n",
        "            # except:\n",
        "            #     pass\n",
        "        pbar = tqdm(total=n_train)\n",
        "        self.reset_state_variables()\n",
        "\n",
        "\n",
        "\n",
        "        for (i, datum) in enumerate(dataloader):\n",
        "            if self.load_path:\n",
        "                #try:\n",
        "                if i <= iteration:\n",
        "                    n_train += 1\n",
        "                    continue\n",
        "                # except:\n",
        "                #     pass\n",
        "            if i > n_train:\n",
        "                break\n",
        "\n",
        "            image = datum[\"encoded_image\"]\n",
        "            if self.label is None : label = datum[\"label\"]\n",
        "\n",
        "            # Run the network on the input.\n",
        "            if gpu:\n",
        "                inputs = {\"input\": image.cuda().view(self.time, 1, self.in_channels, self.crop_size, self.crop_size)}\n",
        "            else:\n",
        "                inputs = {\"input\": image.view(self.time, 1, self.in_channels, self.crop_size, self.crop_size)}\n",
        "            #print(self.spikes['output'].get('s'))\n",
        "\n",
        "            ### Spike clamping (baseline activity)\n",
        "\n",
        "            clamp = {}\n",
        "            if self.clamp_intensity is not None:\n",
        "                encoder = PoissonEncoder(time = self.time, dt = self.dt)\n",
        "                clamp['output'] = encoder.enc(datum = torch.rand(self.layers['output'].n)*self.clamp_intensity,time = self.time, dt = self.dt)\n",
        "\n",
        "            self.run(inputs=inputs, \n",
        "                    time=self.time, \n",
        "                    **reward_hparams,\n",
        "                    one_step=True,\n",
        "                    true_label = label.int().item(),\n",
        "                    dopaminergic_layers= self.dopaminergic_layers,\n",
        "                    clamp = clamp\n",
        "                     )\n",
        "\n",
        "            # Get voltage recording.\n",
        "            #main_voltage = main_monitor.get(\"v\")\n",
        "            reward_history.append(reward_monitor.get())\n",
        "            #tensorboard.update(step= i)\n",
        "\n",
        "            # Add to spikes recording.\n",
        "            #if self.single_output_layer:\n",
        "            lc_spikes1 = self.spikes['main1'].get('s')\n",
        "            #lc_spikes2 = self.spikes['main2'].get('s')\n",
        "            out_spikes = self.spikes[\"output\"].get(\"s\").view(self.time, n_classes, neuron_per_class)\n",
        "            sum_spikes = out_spikes[self.observation_period:self.observation_period+self.decision_period,:,:].sum(0).sum(1)\n",
        "            predicted_label = torch.argmax(sum_spikes)\n",
        "            \n",
        "            # else:\n",
        "            #     spikes_record = torch.zeros(self.n_classes, self.time, self.neuron_per_class)\n",
        "            #     for c in range(self.n_classes):\n",
        "            #         spikes_record[c] = self.output_spikes[f\"output_{c}\"].get(\"s\").squeeze(1)\n",
        "            #     sum_spikes = spikes_record.sum(1).sum(1)\n",
        "            #     predicted_label = torch.argmax(sum_spikes)    \n",
        "\n",
        "            if predicted_label == label:\n",
        "                acc_hist.append(1)\n",
        "            else:\n",
        "                acc_hist.append(0)\n",
        "  \n",
        "            w_lc1 = self.connections[('input', 'main1')].w\n",
        "            w_last_main_out = self.connections[(self.final_connection_source_name,'output')].w\n",
        "            #w_lc2 = self.connections[('main1', 'main2')].w\n",
        "            #w_inh = self.connections[('output','output')].w\n",
        "            convg_lc1=  1 -  torch.mean((w_lc1 - self.wmin)*(self.wmax-w_lc1))\n",
        "            convg_out= 1 -  torch.mean((w_last_main_out - self.wmin)*(self.wmax-w_last_main_out))\n",
        "            if self.norm_factor_LC is not None:\n",
        "                mean_norm_factor_lc = self.norm_factor_LC / w_lc1.shape[-1]\n",
        "                convg_lc1=  1 - ( torch.mean((w_lc1 - self.wmin)*(self.wmax-w_lc1)) / ((mean_norm_factor_lc - self.wmin)*(self.wmax - mean_norm_factor_lc)) )\n",
        "            if self.norm_factor_out is not None:    \n",
        "                mean_norm_factor_out = self.norm_factor_out / w_last_main_out.shape[-2]\n",
        "                convg_out= 1 - ( torch.mean((w_last_main_out - self.wmin)*(self.wmax-w_last_main_out)) / ((mean_norm_factor_out - self.wmin)*(self.wmax - mean_norm_factor_out)) )\n",
        "           \n",
        "            self.convergences['lc1'].append((convg_lc1 * 10**4).round() / (10**4))\n",
        "            self.convergences['last_main_out'].append((convg_out * 10**4).round() / (10**4)) \n",
        "\n",
        "            print(\"\\routput\", sum_spikes, 'pred_label:',\n",
        "                predicted_label.item(), 'GT:', label.item(),\n",
        "                ', Acc Rew:', round(sum(reward_monitor.get()).item(),4),\n",
        "                f\"Pos dps: {self.reward_fn.dps:.5f}, Neg dps: {self.reward_fn.neg_dps:.5f}, Rew base: {self.reward_fn.rew_base:.5f}, Pun base: {self.reward_fn.punish_base:.5f}, RPe: {self.reward_fn.reward_predict_episode:.3f}\",\n",
        "                f\"input_mean_fire_freq: {torch.mean(image.float())*1000:.1f},main_mean_fire_freq:{torch.mean(lc_spikes1.float())*1000:.1f}\",#\" main2_mean_fire_freq:{torch.mean(lc_spikes2.float())*1000:.1f}\",\n",
        "                f\"output_mean_fire_freq:{torch.mean(out_spikes.float())*1000:.1f}\",\n",
        "                f\"mean_lc1_w: {torch.mean(w_lc1):.5f}, mean_fc_w:{torch.mean(w_last_main_out):.5f}\",\n",
        "                f\"std_lc1_w: {torch.std(w_lc1):.5f}, std_fc_w:{torch.std(w_last_main_out):.5f}\",\n",
        "                f\"convergence_lc1: {convg_lc1:.5f}, convergence_fc: {convg_out:.5f}\",\n",
        "                end = '')           \n",
        "            \n",
        "            acc = 100 * sum(acc_hist)/len(acc_hist)\n",
        "            self.reward_fn.update(accumulated_reward= sum(reward_monitor.get()), ema_window = reward_hparams['ema_window']) \n",
        "\n",
        "            if online_validate and i % val_interval == 0 and i!=0:\n",
        "                self.reset_state_variables()\n",
        "                val_acc = self.evaluate(val_loader, n_val, val_interval, running_window_length)\n",
        "                #tensorboard.writer.add_scalars(\"accuracy\", {\"train\": acc, \"val\" : val_acc}, i)\n",
        "                train_accs.append(acc)\n",
        "                val_accs.append(val_acc)\n",
        "                #acc_rewards.append(sum(reward_monitor.get()))\n",
        "                if self.save_path is not None:\n",
        "                    model_params = {'state_dict': self.state_dict(), 'hparams': hparams, 'iteration': i, 'val_accs': val_accs, 'train_accs': train_accs, 'acc_rewards': acc_rewards}\n",
        "                    torch.save(model_params, self.save_path)\n",
        "            # else:\n",
        "            #     pass\n",
        "                #tensorboard.writer.add_scalars(\"accuracy\", {\"train\": acc}, i)\n",
        "            #tensorboard.writer.add_scalar(\"reward\", sum(reward_monitor.get()), i)\n",
        "\n",
        "            # if  i % val_interval == 0 and i!=0:\n",
        "            #     fig = create_plot(self.output_spikes, reward_monitor.get(), label)\n",
        "            #     tensorboard.writer.add_figure('reward', fig, i)\n",
        "            \n",
        "            #Plot_et.plot()    \n",
        "            self.reset_state_variables()  # Reset state variables.\n",
        "            \n",
        "            pbar.set_description_str(\"Running accuracy: \" + \"{:.2f}\".format(acc) + \"%, \" + \"Current val accuracy: \" + \"{:.2f}\".format(val_acc) + \"%, \")\n",
        "            pbar.update()\n",
        "\n",
        "        result_metrics = {'train_acc': acc, 'val_acc': val_acc}\n",
        "        # tensorboard.writer.add_hparams(\n",
        "        #     {k:(v if type(v) in (int, float, bool, str, torch.Tensor) else str(v)) for k,v in {**train_hparams, **data_hparams, **network_hparams, **reward_hparams}.items() },\n",
        "        #     result_metrics\n",
        "        # )\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate(self, val_loader, n_val, val_interval, running_window_length):\n",
        "        manual_seed(SEED)\n",
        "        acc_hist_val = collections.deque([], running_window_length)\n",
        "\n",
        "        spikes_val = {}\n",
        "\n",
        "        self.train(False)\n",
        "        self.learning = False\n",
        "\n",
        "        GT, y_pred = [], []\n",
        "        for (i, datum) in enumerate(val_loader):\n",
        "            if i > n_val:\n",
        "                break\n",
        "\n",
        "            image = datum[\"encoded_image\"]\n",
        "            if self.label is None : \n",
        "              label = datum[\"label\"]\n",
        "            else :\n",
        "              label = self.label\n",
        "\n",
        "            # Run the network on the input.\n",
        "            if gpu:\n",
        "                inputs = {\"input\": image.cuda().view(self.time, 1, self.in_channels, self.crop_size, self.crop_size)}\n",
        "            else:\n",
        "                inputs = {\"input\": image.view(self.time, 1, self.in_channels, self.crop_size, self.crop_size)}\n",
        "\n",
        "            self.run(inputs=inputs, \n",
        "                    time=self.time, \n",
        "                    **reward_hparams,\n",
        "                    one_step = True,\n",
        "                    true_label = label.int().item(),\n",
        "                    dopaminergic_layers= self.dopaminergic_layers,\n",
        "                     )\n",
        "            # Add to spikes recording.\n",
        "            #if self.single_output_layer:\n",
        "            out_spikes = self.spikes[\"output\"].get(\"s\").view(self.time, n_classes, neuron_per_class)\n",
        "            sum_spikes = out_spikes[self.observation_period:self.observation_period+self.decision_period,:,:].sum(0).sum(1)\n",
        "            predicted_label = torch.argmax(sum_spikes)\n",
        "            # else:\n",
        "            #     spikes_record = torch.zeros(self.n_classes, self.time, self.neuron_per_class)\n",
        "            #     for c in range(self.n_classes):\n",
        "            #         spikes_record[c] = self.output_spikes[f\"output_{c}\"].get(\"s\").squeeze(1)\n",
        "            #     sum_spikes = spikes_record.sum(1).sum(1)\n",
        "            #     predicted_label = torch.argmax(sum_spikes)\n",
        "\n",
        "            if predicted_label == label:\n",
        "                acc_hist_val.append(1)\n",
        "            else:\n",
        "                acc_hist_val.append(0)\n",
        "            \n",
        "            GT.append(label)\n",
        "            y_pred.append(predicted_label)\n",
        "            \n",
        "            print(\"\\r*validation: output\",sum_spikes,\n",
        "                'predicted_label:', predicted_label.item(), 'GT:', label.item(),\n",
        "                end = '') if self.verbose else None\n",
        "            \n",
        "            self.reset_state_variables()  # Reset state variables.\n",
        "\n",
        "        self.train(True)\n",
        "        self.learning = True\n",
        "        \n",
        "        if self.confusion_matrix:\n",
        "            self.plot_confusion_matrix(GT, y_pred)\n",
        "\n",
        "        if self.lc_weights_vis:\n",
        "            plot_locally_connected_weights_meh(self.connections[('input','main1')].w,self.n_channels1,self.in_channels,0,self.crop_size,self.filter_size1,self.layers['main1'].shape[1])\n",
        "            plt.show()\n",
        "            if self.deep:\n",
        "                plot_locally_connected_weights_meh(self.connections[(self.hidden2_name,'main1')].w,self.n_channels2,self.n_channels1,0,self.layers[self.hidden2_name].shape[1],self.filter_size2,self.layers['main2'].shape[1])                \n",
        "                plt.show()\n",
        "        \n",
        "        if self.lc_convergence_vis: \n",
        "            plot_convergence_and_histogram(self.connections[('input','main1')].w,self.convergences['lc1'])\n",
        "            plt.show()\n",
        "        if self.out_convergence_vis:\n",
        "            plot_convergence_and_histogram(self.connections[(self.final_connection_source_name,'output')].w,self.convergences['last_main_out'])\n",
        "            plt.show() \n",
        "\n",
        "        if self.out_weights_vis:\n",
        "            plot_locally_connected_weights_meh3(self.connections[('input','main1')].w,self.connections[(self.final_connection_source_name,'output')].w,\\\n",
        "                                                0,0,self.neuron_per_class,self.n_channels1,self.in_channels,0,self.crop_size,self.filter_size1,self.layers['main1'].shape[1]) \n",
        "            plt.show()\n",
        "            plot_locally_connected_weights_meh3(self.connections[('input','main1')].w,self.connections[(self.final_connection_source_name,'output')].w,\\\n",
        "                                                0,1,self.neuron_per_class,self.n_channels1,self.in_channels,0,self.crop_size,self.filter_size1,self.layers['main1'].shape[1])           \n",
        "            plt.show()\n",
        "        val_acc = 100 * sum(acc_hist_val)/len(acc_hist_val)\n",
        "        return val_acc\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_confusion_matrix(GT, y_predicted):\n",
        "        cm = confusion_matrix(GT, y_predicted)\n",
        "        plt.figure(figsize = (10,7))\n",
        "        sn.heatmap(cm, annot=True)\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Truth')\n",
        "        plt.show()\n",
        "\n",
        "    \n",
        "    def one_step(self,datum,label = None):\n",
        "        self.reset_state_variables()\n",
        "        \n",
        "        image = datum[\"encoded_image\"]\n",
        "        if label is None : label = datum[\"label\"]\n",
        "\n",
        "        if gpu:\n",
        "            inputs = {\"input\": image.cuda().view(self.time, 1, self.in_channels, self.crop_size, self.crop_size)}\n",
        "        else:\n",
        "            inputs = {\"input\": image.view(self.time, 1, self.in_channels, self.crop_size, self.crop_size)}\n",
        "\n",
        "        clamp = {}\n",
        "        if self.clamp_intensity is not None:\n",
        "            encoder = PoissonEncoder(time = self.time, dt = self.dt)\n",
        "            clamp['output'] = encoder.enc(datum = torch.rand(self.layers['output'].n)*self.clamp_intensity,time = self.time, dt = self.dt)\n",
        "\n",
        "        self.run(inputs=inputs, \n",
        "                time=self.time, \n",
        "                **reward_hparams,\n",
        "                one_step = True,\n",
        "                true_label = label.int().item(),\n",
        "                dopaminergic_layers= self.dopaminergic_layers,\n",
        "                )\n",
        "    "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zuUBU9pU3vE"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DW7dB11jdqi"
      },
      "source": [
        "class ClassSelector(torch.utils.data.sampler.Sampler):\n",
        "    \"\"\"Select target classes from the dataset\"\"\"\n",
        "    def __init__(self, target_classes, data_source, mask = None):\n",
        "        if mask is not None:\n",
        "            self.mask = mask\n",
        "        else:\n",
        "            self.mask = torch.tensor([1 if data_source[i]['label'] in target_classes else 0 for i in range(len(data_source))])\n",
        "        self.data_source = data_source\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter([i.item() for i in torch.nonzero(self.mask)])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_source)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlCXBY0DU3Mc"
      },
      "source": [
        "kernels = [DoGKernel(7,1,2),\n",
        "\t\t\tDoGKernel(7,2,1),]\n",
        "filter = Filter(kernels, padding = 3, thresholds = 50/255)\n",
        "# Load MNIST data.\n",
        "\n",
        "def load_datasets(network_hparams, data_hparams, mask=None, test_mask=None):\n",
        "    manual_seed(SEED)\n",
        "    dataset = MNIST(\n",
        "        PoissonEncoder(time=network_hparams['time'], dt=network_hparams['dt']),\n",
        "        None,\n",
        "        root=os.path.join(\"..\", \"..\", \"data\", \"MNIST\"),\n",
        "        download=True,\n",
        "        transform=transforms.Compose(\n",
        "            [transforms.ToTensor(),\n",
        "            # filter,\n",
        "            transforms.Lambda(lambda x: (\n",
        "                x.round() if data_hparams['round_input'] else x\n",
        "            ) * data_hparams['intensity']),\n",
        "            transforms.CenterCrop(data_hparams['crop_size'])]\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Create a dataloader to iterate and batch data\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1,\n",
        "                                            sampler = ClassSelector(\n",
        "                                                    target_classes = target_classes,\n",
        "                                                    data_source = dataset,\n",
        "                                                    mask = mask,\n",
        "                                                    ) if target_classes else None\n",
        "                                            )\n",
        "\n",
        "    # Load test dataset\n",
        "    test_dataset = MNIST(   \n",
        "        PoissonEncoder(time=network_hparams['time'], dt=network_hparams['dt']),\n",
        "        None,\n",
        "        root=os.path.join(\"..\", \"..\", \"data\", \"MNIST\"),\n",
        "        download=True,\n",
        "        train=False,\n",
        "        transform=transforms.Compose(\n",
        "            [transforms.ToTensor(),\n",
        "            # filter,\n",
        "            transforms.Lambda(lambda x: (\n",
        "                x.round() if data_hparams['round_input'] else x\n",
        "            ) * data_hparams['intensity']),\n",
        "            transforms.CenterCrop(data_hparams['crop_size'])]\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1,\n",
        "                                            sampler = ClassSelector(\n",
        "                                                    target_classes = target_classes,\n",
        "                                                    data_source = test_dataset,\n",
        "                                                    mask = mask_test,\n",
        "                                                    ) if target_classes else None\n",
        "                                            )\n",
        "    \n",
        "\n",
        "    return dataloader, val_loader"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCqAFucAUDb8"
      },
      "source": [
        "# Set up hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0Wh_eJIFkO-"
      },
      "source": [
        "train_hparams = {\n",
        "    'n_train' : 3000,\n",
        "    'n_test' : 3000,\n",
        "    'n_val' : 1,\n",
        "    'val_interval' : 200,\n",
        "    'running_window_length': 250,\n",
        "}"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVOqxcYtFd5T"
      },
      "source": [
        "# Dataset Hyperparameters\n",
        "target_classes = None #(0,1)\n",
        "if target_classes:\n",
        "    npz_file = np.load(f'bindsnet/mask_{\"_\".join([str(i) for i in target_classes])}.npz')\n",
        "    # npz_file = np.load('bindsnet/mask_0_1.npz') ##### KESAFAT KARI !!!\n",
        "    mask, mask_test = torch.from_numpy(npz_file['arr_0']), torch.from_numpy(npz_file['arr_1'])\n",
        "    n_classes = len(target_classes)\n",
        "    \n",
        "else:\n",
        "    mask = None\n",
        "    mask_test = None\n",
        "    n_classes = 10\n",
        "\n",
        "data_hparams = { \n",
        "    'intensity': 128,\n",
        "    'crop_size': 20,\n",
        "    'round_input': False,\n",
        "}"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TerGeJoFdzg"
      },
      "source": [
        "n_neurons = 500 #100\n",
        "neuron_per_class = int(n_neurons/n_classes)\n",
        "single_output_layer = True\n",
        "thresh_LC = -52\n",
        "thresh_FC = -52\n",
        "\n",
        "network_hparams = {\n",
        "    # net structure\n",
        "    'crop_size': 20,\n",
        "    'neuron_per_class': neuron_per_class,\n",
        "    'deep': False,\n",
        "    'maxPool1': False,\n",
        "    'maxPool2': False,\n",
        "    'in_channels':1,\n",
        "    'n_channels1': 100,\n",
        "    'n_channels2': 64,\n",
        "    'filter_size1': 12,\n",
        "    'filter_size2': 5,\n",
        "    'stride1': 4,\n",
        "    'stride2': 1,\n",
        "    'n_neurons' : n_neurons,\n",
        "    'n_classes': n_classes,\n",
        "    'single_output_layer': single_output_layer,\n",
        "    \n",
        "    # time & Phase\n",
        "    'dt' : 1,\n",
        "    'pre_observation': True,\n",
        "    'has_decision_period': True,\n",
        "    'observation_period': 100,\n",
        "    'decision_period': 250,\n",
        "    'online': False,\n",
        "    'local_rewarding': False,\n",
        "     \n",
        "    # Nodes\n",
        "    'NodesType_LC': AdaptiveLIFNodes,\n",
        "    'NodesType_Output': LIFNodes, \n",
        "    'theta_plus': 0.05,\n",
        "    'tc_theta_decay': 1e6,\n",
        "    'tc_trace':20,\n",
        "    'trace_additive' : False,\n",
        "    \n",
        "    # Learning\n",
        "    'update_rule_LC': None,\n",
        "    'update_rule_LC2': None,\n",
        "    'update_rule_Output': MSTDP,\n",
        "    'update_rule_inh': None,\n",
        "    'update_rule_inh_LC' : None,\n",
        "    'nu_LC': (0.00,0.0),\n",
        "    'nu_LC2': (0.0,0.0),\n",
        "    'nu_Output':0.1,\n",
        "    'nu_inh': 0.0,\n",
        "    'nu_inh_LC': 0.0,\n",
        "    'soft_bound': False,\n",
        "    'thresh_LC': thresh_LC,\n",
        "    'thresh_FC': thresh_FC,\n",
        "\n",
        "    # weights\n",
        "    'normal_init': False,\n",
        "    'mu' : 0.8,\n",
        "    'std' : 0.02,\n",
        "    'wmin': 0.0,\n",
        "    'wmax': 1.0,\n",
        "    \n",
        "    # Inhibition\n",
        "    'inh_type': 'between_layers',\n",
        "    'inh_factor': 100,\n",
        "    'inh_LC': True,\n",
        "    'inh_factor_LC': 100,\n",
        "    'inh_LC2': False,\n",
        "    'inh_factor_LC2': 0,\n",
        "    \n",
        "    # Normalization\n",
        "    'norm_factor_LC': 0.2*12*12,\n",
        "    'norm_factor_LC2': None,\n",
        "    'norm_factor_out': None,\n",
        "    'norm_factor_inh': None,\n",
        "    'norm_factor_inh_LC': None,\n",
        "    \n",
        "    # clamp\n",
        "    'clamp_intensity': None,#1000,\n",
        "\n",
        "    # Save\n",
        "    'save_path': '/content/drive/My Drive/LCNet/LCNet_again_pre.pth',\n",
        "    'load_path': '/content/drive/My Drive/LCNet/LCNet_again_pre.pth',\n",
        "    'LC_weights_path': '/content/drive/My Drive/LCNet/LCNet_again.pth',#'/content/drive/My Drive/LCNet/LCNet_ch81_f13_22_2norm_Adapt_fc_test2.pth',\n",
        "    'LC2_weights_path': None,#'/content/drive/My Drive/LCNet/DeepLCNet_layer2_ch64_f5_s2_norm3.pth',\n",
        "\n",
        "    # Plot:\n",
        "    'confusion_matrix' : True,\n",
        "    'lc_weights_vis': True,\n",
        "    'out_weights_vis': True,\n",
        "    'lc_convergence_vis': False,\n",
        "    'out_convergence_vis': True,\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "reward_hparams= {\n",
        "    'n_labels': n_classes,\n",
        "    'neuron_per_class': neuron_per_class,\n",
        "    \n",
        "    'variant': 'scalar',  #true_pred, #pure_per_spike (Just in phase I, online : True) , and #scalar #per_spike\n",
        "    'tc_reward':0,\n",
        "    'dopamine_base': 0.0,\n",
        "    'reward_base': 1.,\n",
        "    'punishment_base': 1.,\n",
        "    \n",
        "\n",
        "    'sub_variant': 'static', #static, #RPE, #pred_decay\n",
        "    'td_nu': 0.0001,  #RPE\n",
        "    'ema_window': 10, #RPE\n",
        "    'tc_dps': 20,     #pred_decay\n",
        "    'dps_factor': 20, #pred_decay, #RPE\n",
        "    }\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Rz1wmKKYuVg",
        "outputId": "cefbab80-2339-4c0e-d349-02245ac74e6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "time = 600 #320\n",
        "network_hparams.update(\n",
        "    {\n",
        "     'time': time,\n",
        "    }\n",
        ")\n",
        "dataloader, val_loader = load_datasets(network_hparams, data_hparams, mask, mask_test)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Using downloaded and verified file: ../../data/MNIST/TorchvisionDatasetWrapper/raw/train-images-idx3-ubyte.gz\n",
            "Extracting ../../data/MNIST/TorchvisionDatasetWrapper/raw/train-images-idx3-ubyte.gz to ../../data/MNIST/TorchvisionDatasetWrapper/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Using downloaded and verified file: ../../data/MNIST/TorchvisionDatasetWrapper/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting ../../data/MNIST/TorchvisionDatasetWrapper/raw/train-labels-idx1-ubyte.gz to ../../data/MNIST/TorchvisionDatasetWrapper/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Using downloaded and verified file: ../../data/MNIST/TorchvisionDatasetWrapper/raw/t10k-images-idx3-ubyte.gz\n",
            "Extracting ../../data/MNIST/TorchvisionDatasetWrapper/raw/t10k-images-idx3-ubyte.gz to ../../data/MNIST/TorchvisionDatasetWrapper/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Using downloaded and verified file: ../../data/MNIST/TorchvisionDatasetWrapper/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting ../../data/MNIST/TorchvisionDatasetWrapper/raw/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/TorchvisionDatasetWrapper/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SokdidkrV2Z5"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Venb2KhSYrT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77afd414-c698-40f4-875a-dcd148620663"
      },
      "source": [
        "from google.colab import drive\n",
        "if network_hparams['save_path'] or network_hparams['LC_weights_path']:    \n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRruWKq0kHbz"
      },
      "source": [
        "### Hyperparameters 1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IavVVHrvYVoI"
      },
      "source": [
        "manual_seed(SEED)\n",
        "hparams = {**reward_hparams, **network_hparams, **train_hparams, **data_hparams}\n",
        "net = LCNet(**hparams, reward_fn = DynamicDopamineInjection)\n",
        "net.fit(dataloader = dataloader, val_loader = val_loader, reward_hparams = reward_hparams, **train_hparams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfTcVmMNCSUo"
      },
      "source": [
        "plot_locally_connected_weights_meh(net.connections[('input', 'main1')].w,25,1,0,22,13,4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqb0Ph5AYdRv"
      },
      "source": [
        "dataloader_iterator = iter(dataloader)\n",
        "for i in range(10):\n",
        "    datum = next(dataloader_iterator)\n",
        "    plt.imshow(datum['encoded_image'].squeeze().sum(0))\n",
        "    plt.show()\n",
        "    net.one_step(datum)\n",
        "    spikes = net.spikes[\"main1\"].get(\"s\").sum(0).squeeze().view(4*5,4*5)\n",
        "    print(torch.max(spikes))\n",
        "    plot_locally_connected_weights_meh2(net.connections[('input', 'main1')].w,spikes,25,1,0,22,13,4)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz4jl4_2jso-"
      },
      "source": [
        "# plot_locally_connected_weights_meh3(self.connections[('input','main1')].w,self.connections[(self.final_connection_source_name,'output')].w,\\\n",
        "#                                     0,0,self.neuron_per_class,self.n_channels1,self.in_channels,0,self.crop_size,self.filter_size1,self.layers['main1'].shape[1]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkz7skWaiWj7"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAKGbShj7kOM"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXv13w317oW7"
      },
      "source": [
        "%tensorboard --logdir '/content/runs'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGe49P4-q9zp"
      },
      "source": [
        "## Save/Load Sessions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ow_r8C5qzwu"
      },
      "source": [
        "Save tensorBoard Session "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdHDClhIqUlW"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp -a /content/runs/. /content/drive/MyDrive/LCNet/logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NOgZCamq4p-"
      },
      "source": [
        "Read Saved Sessions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlItthjkq4S5"
      },
      "source": [
        "%tensorboard --logdir /content/drive/MyDrive/LCNet/logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZtdOQm4oAu1"
      },
      "source": [
        "## Optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcLDxcrwithj"
      },
      "source": [
        "install and import optuna"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGik9_MeQfaI"
      },
      "source": [
        "!pip install optuna\n",
        "import optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsM-Y9CZiv15"
      },
      "source": [
        "Define objective function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj_n_rwWDrdV"
      },
      "source": [
        "STUDY_NAME  = ''\n",
        "DATA_PATH = ''\n",
        "N_TRIALS = ''\n",
        "\n",
        "def objective(trial):\n",
        "    ### Suggest parameters: \n",
        "    num_layers = trial.suggest_int('Number of Layers', 1, 4)\n",
        "    dropout_rate  = trial.suggest_float('Dropout', 0, .99)\n",
        "    activation = trial.suggest_categorical('activation', ['relu', 'selu', 'sigmoid', 'elu'])\n",
        "    lr = trial.suggest_float('Learning rate', 1e-6, 1)\n",
        "    network_hparams.update({\n",
        "        \n",
        "    })\n",
        "    ###Define your model\n",
        "    manual_seed(SEED)\n",
        "    hparams = {**reward_hparams, **network_hparams, **train_hparams, **data_hparams}\n",
        "    net = LCNet(**hparams, reward_fn = DynamicDopamineInjection)\n",
        "    va_acc = net.fit(dataloader = dataloader, val_loader = val_loader, reward_hparams = reward_hparams, **train_hparams)\n",
        "    ### Define objective value\n",
        "    objective_value = min(va_acc)\n",
        "\n",
        "    \n",
        "    return objective_value\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywzhxB84jthL"
      },
      "source": [
        "Run the study"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDFttXEkjv4f"
      },
      "source": [
        "study = optuna.create_study(study_name = STUDY_NAME , storage=f\"sqlite:////content/drive/MyDrive/LCNet/optuna/optuna_study.db\", load_if_exists=True)\n",
        "study.optimize(objective, n_trials=N_TRIALS )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Azp25HveO77U",
        "outputId": "5fdaa0eb-5888-4acd-acdb-d059bfcd5b87"
      },
      "source": [
        "study.best_params"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Dropout': 0.09142336347778651,\n",
              " 'Layer 1': 82,\n",
              " 'Layer 2': 508,\n",
              " 'Layer 3': 286,\n",
              " 'Learning rate': 0.008771184760927113,\n",
              " 'Number of Layers': 3,\n",
              " 'activation': 'relu',\n",
              " 'l1': None}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epsdhrYujy4f"
      },
      "source": [
        "Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYgaOQZBaPJk"
      },
      "source": [
        "plot_parallel_coordinate(study, params=[\"Learning rate\", \"Number of Layers\", \"Dropout\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya3ZmsSUqtd3"
      },
      "source": [
        "## Izhikevich 2007 - Task 2 | Pavlovian Conditioning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw9OfurBUytc"
      },
      "source": [
        "train_hparams = {\n",
        "    'n_train' : 4000,\n",
        "    'n_test' : 500,\n",
        "    'n_val' : 50,\n",
        "    'val_interval' : 250,\n",
        "    'running_window_length': 250,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK13AFE7Uytc"
      },
      "source": [
        "# Dataset Hyperparameters\n",
        "target_classes = (8,9)\n",
        "if target_classes:\n",
        "    #npz_file = np.load(f'bindsnet/mask_{\"_\".join([str(i) for i in target_classes])}.npz')\n",
        "    npz_file = np.load('bindsnet/mask_5.npz') ##### KESAFAT KARI !!!\n",
        "    mask, mask_test = torch.from_numpy(npz_file['arr_0']), torch.from_numpy(npz_file['arr_1'])\n",
        "    n_classes = len(target_classes)\n",
        "    \n",
        "else:\n",
        "    mask = None\n",
        "    mask_test = None\n",
        "    n_classes = 10\n",
        "\n",
        "data_hparams = { \n",
        "    'intensity': 128,\n",
        "    'crop_size': 22,\n",
        "    'round_input': False,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rqj7BASfUytd"
      },
      "source": [
        "n_neurons = 2 #100\n",
        "neuron_per_class = int(n_neurons/n_classes)\n",
        "single_output_layer = True\n",
        "\n",
        "network_hparams = {\n",
        "    # net structure\n",
        "    'crop_size': 22,\n",
        "    'neuron_per_class': neuron_per_class,\n",
        "    'deep': False,\n",
        "    'maxPool1': False,\n",
        "    'maxPool2': False,\n",
        "    'in_channels':1,\n",
        "    'n_channels1': 25,\n",
        "    'n_channels2': 64,\n",
        "    'filter_size1': 13,\n",
        "    'filter_size2': 5,\n",
        "    'stride1': 3,\n",
        "    'stride2': 1,\n",
        "    'n_neurons' : n_neurons,\n",
        "    'n_classes': n_classes,\n",
        "    'single_output_layer': single_output_layer,\n",
        "    \n",
        "    # time & Phase\n",
        "    'dt' : 1,\n",
        "    'pre_observation': False,\n",
        "    'has_decision_period': True,\n",
        "    'observation_period': 0,\n",
        "    'decision_period': 256,\n",
        "    'online': False,\n",
        "    'local_rewarding': False,\n",
        "     \n",
        "    # Nodes\n",
        "    'NodesType_LC': AdaptiveLIFNodes,\n",
        "    'NodesType_Output': LIFNodes, \n",
        "    'theta_plus': 0.05,\n",
        "    'tc_theta_decay': 1e6,\n",
        "    'tc_trace':20,\n",
        "    'trace_additive' : False,\n",
        "    \n",
        "    # Learning\n",
        "    'update_rule_LC': None,\n",
        "    'update_rule_LC2': None,\n",
        "    'update_rule_Output': MSTDP,\n",
        "    'update_rule_inh': None,\n",
        "    'update_rule_inh_LC' : None,\n",
        "    'nu_LC': (0.00,0.0),\n",
        "    'nu_LC2': (0.0,0.0),\n",
        "    'nu_Output':0.01,\n",
        "    'nu_inh': 0.0,\n",
        "    'nu_inh_LC': 0.0,\n",
        "    'soft_bound': False,\n",
        "\n",
        "    # weights\n",
        "    'normal_init': False,\n",
        "    'mu' : 0.8,\n",
        "    'std' : 0.05,\n",
        "    'wmin': 0.0,\n",
        "    'wmax': 1.0,\n",
        "    \n",
        "    # Inhibition\n",
        "    'inh_type': 'between_layers',\n",
        "    'inh_factor': 1,\n",
        "    'inh_LC': True,\n",
        "    'inh_factor_LC': 100,\n",
        "    'inh_LC2': False,\n",
        "    'inh_factor_LC2': 0,\n",
        "    \n",
        "    # Normalization\n",
        "    'norm_factor_LC': 0.25*13*13,\n",
        "    'norm_factor_LC2': None,\n",
        "    'norm_factor_out': None,\n",
        "    'norm_factor_inh': None,\n",
        "    'norm_factor_inh_LC': None,\n",
        "    \n",
        "    # clamp\n",
        "    'clamp_intensity': None,#1000,\n",
        "\n",
        "    # Save\n",
        "    'save_path': None,#'/content/drive/My Drive/LCNet/LCNet_phase2_baseline2_gpu.pth',\n",
        "    'load_path': None,#'/content/drive/My Drive/LCNet/LCNet_phase2_baseline2_gpu.pth',\n",
        "    'LC_weights_path': '/content/drive/My Drive/LCNet/BioLCNet_layer1_Shallow_f13_s3_inh100_norm3.pth',#'/content/drive/My Drive/LCNet/LCNet_ch81_f13_22_2norm_Adapt_fc_test2.pth',\n",
        "    'LC2_weights_path': None,#'/content/drive/My Drive/LCNet/DeepLCNet_layer2_ch64_f5_s2_norm3.pth',\n",
        "\n",
        "    # Plot:\n",
        "    'confusion_matrix' : False,\n",
        "    'lc_weights_vis': False,\n",
        "    'out_weights_vis': True,\n",
        "    'lc_convergence_vis': False,\n",
        "    'out_convergence_vis': True,\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "reward_hparams= {\n",
        "    'n_labels': n_classes,\n",
        "    'neuron_per_class': neuron_per_class,\n",
        "    \n",
        "    'variant': 'scalar',  #true_pred, #pure_per_spike (Just in phase I, online : True) , and #scalar #per_spike\n",
        "    'tc_reward':0,\n",
        "    'dopamine_base': 0.0,\n",
        "    'reward_base': 2.5,\n",
        "    'punishment_base': 2.5,\n",
        "    \n",
        "\n",
        "    'sub_variant': 'static', #static, #RPE, #pred_decay\n",
        "    'td_nu': 0.0001,  #RPE\n",
        "    'ema_window': 10, #RPE\n",
        "    'tc_dps': 20,     #pred_decay\n",
        "    'dps_factor': 20, #pred_decay, #RPE\n",
        "    }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2iFc_v_Uytd"
      },
      "source": [
        "time = 356 #320\n",
        "network_hparams.update(\n",
        "    {\n",
        "     'time': time,\n",
        "    }\n",
        ")\n",
        "dataloader, val_loader = load_datasets(network_hparams, data_hparams, mask, mask_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGsvklk7U7_r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3cebe3b-f481-4d7c-eea0-06a2a6a21f46"
      },
      "source": [
        "from google.colab import drive\n",
        "if network_hparams['save_path'] or network_hparams['LC_weights_path']:    \n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaBPo8BJU7_s"
      },
      "source": [
        "manual_seed(SEED)\n",
        "hparams = {**reward_hparams, **network_hparams, **train_hparams, **data_hparams}\n",
        "net = LCNet(**hparams, reward_fn = DynamicDopamineInjection)\n",
        "net.fit(dataloader = dataloader, label = torch.tensor([1]),val_loader = val_loader, reward_hparams = reward_hparams, **train_hparams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy7-iAETvuPH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439,
          "referenced_widgets": [
            "661194ee80564effb4e50a9317318c7a",
            "92194849783e4c0b960c23ca86006a10",
            "98fc35d9ecca410f8b515b11c7260b43",
            "fdda91ea19b3465a9ccf453cc2a36c15",
            "7a08fbda807c40c79b41433ccc759803",
            "672be7dc1f1041ecbe4d8e6d2e6554e3",
            "ea4a9292abf5464ca25e407a262b7854",
            "db144e915cb14c418d82b2f9107cef01",
            "b69ab972c8a842f9aacebba80d5554b7",
            "6bf1b8ee9ecc4330a35777de7a491df5",
            "9042967f47494e5a92663c30648fcb74"
          ]
        },
        "outputId": "42a308c1-e219-4b2f-fae6-b02090072a5b"
      },
      "source": [
        "net.fit(dataloader = dataloader, val_loader = val_loader, reward_hparams = reward_hparams, hparams = hparams, **train_hparams)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin training.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "661194ee80564effb4e50a9317318c7a",
              "version_minor": 0,
              "version_major": 2
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output tensor([878,  95]) pred_label: 0 GT: 0 , Acc Rew: 220.4388 Pos dps: 1.00000, Neg dps: 1.00000, Rew base: 0.88175, Pun base: 1.11825, RPe: 135.512 input_mean_fire_freq: 34.3,main_mean_fire_freq:105.3 output_mean_fire_freq:38.4 mean_lc_w: 0.97072, mean_fc_w:0.23497 std_lc_w: 0.10048,std_fc_w:0.32059"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-22a2e2b2dc29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_hparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_hparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_hparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_hparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrain_hparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-3ee30e0b7704>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataloader, val_loader, reward_hparams, hparams, online_validate, n_train, n_test, n_val, val_interval, running_window_length, verbose)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mmain_voltage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_monitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"v\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mreward_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_monitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m             \u001b[0;31m# Add to spikes recording.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;31m#if self.single_output_layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bindsnet/network/monitors.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, step)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_scalers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bindsnet/network/monitors.py\u001b[0m in \u001b[0;36m_add_grids\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m                         \u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m                         \u001b[0mdataformats\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'HW'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m                         )\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36madd_image\u001b[0;34m(self, tag, img_tensor, global_step, walltime, dataformats)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0mimg_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFetchBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         self._get_file_writer().add_summary(\n\u001b[0;32m--> 549\u001b[0;31m             image(tag, img_tensor, dataformats=dataformats), global_step, walltime)\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwalltime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataformats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NCHW'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/tensorboard/summary.py\u001b[0m in \u001b[0;36mimage\u001b[0;34m(tag, tensor, rescale, dataformats)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;31m# Do not assume that user passes in values in [0, 255], use data type to detect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0mscale_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_calc_scale_factor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrbB8W8g7JbQ"
      },
      "source": [
        "for i in range(20):\n",
        "  net.one_step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkmj9tDazbm3"
      },
      "source": [
        "## Plotting the feature maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSdD8hPAzm5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98c2cf40-d558-43e5-94f3-c370d603f37f"
      },
      "source": [
        "n_filts = (int((network_hparams['crop_size']-network_hparams['filter_size'])/network_hparams['stride'])+1)\n",
        "n_filts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 444
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYAtejQAzoku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52685884-8bad-4b58-b492-333f1f708a19"
      },
      "source": [
        "\n",
        "\n",
        "print(net.connections[('input', 'main')].w.shape)\n",
        "\n",
        "in_chans = 2\n",
        "n_chans = 100\n",
        "k = 12\n",
        "\n",
        "reshaped_w = net.connections[('input', 'main')].w.view(in_chans, n_chans, n_filts, n_filts, k, k)\n",
        "\n",
        "print(reshaped_w.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 900, 144])\n",
            "torch.Size([2, 100, 3, 3, 12, 12])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut7wA1Emzq4J"
      },
      "source": [
        "chan_idx = 20 # 0 to N_channels \n",
        "fig, axs = plt.subplots(n_filts, n_filts)\n",
        "for i in range(n_filts):\n",
        "    for j in range(n_filts):\n",
        "        axs[i][j].imshow(reshaped_w[0,chan_idx,i,j].view(k, k).cpu(), cmap='Greys')\n",
        "        axs[i][j].axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}